{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to NLP and Basic Concepts**"
      ],
      "metadata": {
        "id": "umMFeeEDcY2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is NLP (Natural Language Processing)?\n",
        "\n",
        "NLP is a field of artificial intelligence that enables computers to understand, analyze and interpret human language. NLP is used in many tasks such as understanding language structures, analyzing texts, performing emotion analysis, and providing translation. NLP combines linguistic rules, machine learning and deep learning methods to unravel the complexity of language."
      ],
      "metadata": {
        "id": "S2tPBMTmdWBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization (Word/Phrase Division):**\n",
        "   \n",
        "   Tokenization is the process of breaking a text into smaller pieces, usually words or sentences. This is done before we start analyzing the structure of the language."
      ],
      "metadata": {
        "id": "phIbDvVEdyeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am9tDitregw9",
        "outputId": "c4332408-9041-4034-f931-89878c7afb0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=input(\"Enter a sentence: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhv7ZRtGcawZ",
        "outputId": "75327fae-847c-4304-c099-efdb91ca4623"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence: Today is the first day of the NLP lesson.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8b64q1Hcayz",
        "outputId": "45eeaf1a-843c-457a-edd0-ca97aec88ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Today', 'is', 'the', 'first', 'day', 'of', 'the', 'NLP', 'lesson', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Words:**\n",
        "   \n",
        "   Stop words are words that are frequently used in the language but generally do not have any meaning in terms of analysis. More meaningful analyzes can be made by removing these words (such as and, with, or, etc.) from the text."
      ],
      "metadata": {
        "id": "Yy-0YWUde2PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Sud50Wguxz",
        "outputId": "cd0b1b24-d858-4874-fb3b-8002f8529f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "zZ1j9R6Eca1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lzll6zVgplC",
        "outputId": "86614595-d528-4ea5-d2b4-36f4a57dc64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Today is the 1. day of the our NLP lesson.\"\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "pJZi5kluca6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "words_filtered = []\n",
        "\n",
        "for w in words:\n",
        "  if w not in stopwords:\n",
        "    words_filtered.append(w)\n",
        "\n",
        "print(words_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPivUBu-ca8m",
        "outputId": "08ac1338-886c-483e-f85a-ff3f0e087e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Today', 'first', 'day', 'NLP', 'lesson', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization and Stemming (Finding Root):**\n",
        "   \n",
        "   **Stemming:** It is the process of reducing words to their roots. During this process, the grammatical structure of the word is ignored and the root of the word is tried to be found. <br>\n",
        "   **Lemmatization:** It is the reduction of words to their root form (lemma) in the dictionary. This process preserves the grammatical structure of the word and generally gives results that comply with the rules of the language."
      ],
      "metadata": {
        "id": "_4J4t_d2igBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,WordNetLemmatizer"
      ],
      "metadata": {
        "id": "VpTnjMU9iS54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Running studies caring happier better good\"\n",
        "words = word_tokenize(text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeIdtEA6iS7_",
        "outputId": "c3a26ece-33a4-4b7b-b0bf-c79d9adee7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Running', 'studies', 'caring', 'happier', 'better', 'good']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "stemmer=PorterStemmer()\n",
        "stemmer_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmer words:\",stemmer_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c50eNDQiS-g",
        "outputId": "1472a099-65ef-4c87-ee8d-c9abafcd932b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmer words: ['run', 'studi', 'care', 'happier', 'better', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEhZv3-mj_hk",
        "outputId": "08987102-1d9d-43e9-afb0-4c6a407d566e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"Lemmatized words:\",lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEtlVrJ0iTAo",
        "outputId": "9c3d5e1a-1c81-41f0-95da-8746d5f8e7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized words: ['Running', 'study', 'caring', 'happier', 'better', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result** <br>\n",
        "Stemming: Applies simple rules to get the root form of the word. It's fast but doesn't always create meaningful roots. <br>\n",
        "Lemmatization: Considers the meaning of the word and reduces it to its correct and meaningful form in the dictionary. It gives more accurate results, but it is a little slower and sometimes requires word type information."
      ],
      "metadata": {
        "id": "bSOMQ3PUkV5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning and Preprocessing**"
      ],
      "metadata": {
        "id": "0dNCc7LDkqeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text cleaning and preprocessing is a critical step in natural language processing (NLP) projects to make data usable. This step creates the clean and organized data set necessary for analysis and modeling of the text.\n",
        "\n",
        "**Removing Punctuation Marks:** <br>\n",
        "Punctuation marks are often meaningless in text analysis and are omitted. For example, ,, ., !, ?\n",
        "\n",
        "**Lower-Capital Case Conversions:** <br>\n",
        "Converting all letters in the text to lowercase or uppercase puts the text into a standard form. This ensures that \"NLP\" and \"nlp\" are treated the same.\n",
        "\n",
        "**Extracting Numbers and Special Characters:** <br>\n",
        "Numbers are often not meaningful to analysis and are removed. Likewise, special characters such as @, , $ in the text are also removed."
      ],
      "metadata": {
        "id": "Dlwa3clnkzrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello World! This is a great opportunity to learn NLP: 2024.\"\n",
        "cleaned_text = re.sub(r'[^\\w\\s]','',text) # remove punctuation\n",
        "cleaned_text = re.sub(r'\\d+','',cleaned_text) # remove numbers\n",
        "cleaned_text = cleaned_text.lower() # convert to lowercase\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SYtgwnFiTCw",
        "outputId": "f0c0c1fc-a93a-4912-83ce-f40563792213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world this is a great opportunity to learn nlp \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: <br>\n",
        "\n",
        "*   https://medium.com/@abhishekjainindore24/all-about-tokenization-stop-words-stemming-and-lemmatization-in-nlp-1620ffaf0f87\n"
      ],
      "metadata": {
        "id": "2FOK1WniZOiL"
      }
    }
  ]
}